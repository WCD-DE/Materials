<div  align='left'><img src='https://s3.amazonaws.com/weclouddata/images/logos/wcd_logo_new_2.png' width='15%'></div >

<p style="font-size:30px;text-align:left"><b>Lab Solution--Web Scraping</b></p>
<p style="font-size:20px;text-align:left"><b><font color='#F39A54'>Data Engineering Diploma</font></b></p>
<p style="font-size:15px;text-align:left">Content developed by: WeCloudData Academy</p>
<br>
---
# Project Description: IMDb Top 250 Movies Scraper

## Overview

Welcome to the IMDb Top 250 Movies Scraper project! In this hands-on workshop, you will gain practical experience with web scraping using Scrapy, a powerful and fast asynchronous framework written in Python. Our objective is to create a functioning scraper that can navigate through IMDb's Top 250 movies chart, extract valuable movie information, and store it in a structured format.

By the end of this project, you will have developed a Scrapy spider capable of:

-   Navigating to the IMDb Top 250 Movies page.
-   Extracting movie titles, release years, and ratings.
-   Writing the scraped data to a CSV file.

## Learning Outcomes

This project is designed to solidify your understanding of web scraping concepts and give you the ability to:

-   Set up a Python virtual environment for project isolation.
-   Install and configure Scrapy within a Linux-based development environment.
-   Write Scrapy Spiders to crawl a website and extract data using XPath selectors.
-   Manage the data output by saving the file in a CSV format.

## Project Structure

The project is structured into a series of sequential steps that guide you from setting up your environment to writing and running a Scrapy spider. Each step is accompanied by detailed instructions and commands, as well as a screenshot guide to visually assist you through the process.

## Prerequisites

Before you begin, make sure you have a basic understanding of the following:

-   Python programming language
-   Command-line basics in Linux
-   Fundamental concepts of HTML and web structure
-   XPath query language for selecting nodes in an XML document

## Tools and Resources

Throughout this project, we will be using Scrapy and a series of Linux command-line tools. All required software and libraries will be installed as part of the project steps.

## Expected Outcomes

Upon completing this workshop, you will have a fully functional IMDb movie scraper that you can further customize and expand. Moreover, you will possess the skills to tackle a wide range of web scraping tasks using Scrapy, empowering you to collect data from various websites for your future projects or data analysis needs.

Let's begin!

## Project Execution Steps 
Follow these detailed steps to build and run your IMDb Top 250 Movies Scraper using Scrapy.
### Step 1: Scrapy Installation and Setup

Let's start by setting up our environment. Scrapy will be installed within a virtual environment to avoid dependency conflicts.

1.  **Navigate to your project directory**:
    
    Create a new directory named `imdb_scraper` and move into it:
    
    ```bash
    # Create a new directory
    mkdir imdb_scraper
    # Move into the directory
    cd imdb_scraper
    ``` 
    
    
2.  **Create a virtual environment**:
    
    This encapsulates our project's Python dependencies. Run the following commands in your terminal:
   
    ```bash
    # Create a virtual environment named 'venv'
    python -m venv venv
    # Activate the virtual environment
    source venv/bin/activate
    ``` 
    
   
    
3.  **Install Scrapy**:
    
    With the virtual environment activated, install Scrapy using pip:
        
    ```bash
    # Install Scrapy within the virtual environment
    pip install scrapy
    ``` 
    
 
    

### Step 2: Creating a Scrapy Project

Now it's time to create our new Scrapy project which will house our spider and settings.

1.  **Initialize a new Scrapy project**:
    
    Use the `startproject` command followed by the project name `imdb_scraper`:
    
    ```bash
    # Create a new Scrapy project
    scrapy startproject imdb_scraper
    # Move into the project directory
    cd imdb_scraper
    ``` 
    
    
    

### Step 3: Writing the Spider to extract first title's details

1. Generate a New Spider

    Create a spider named `imdb_top` using the Scrapy `genspider` command:

    `scrapy genspider imdb_top imdb.com` 

    This command generates a spider with `imdb.com` as the allowed domain and creates the `imdb_top.py` file in the `spiders` directory of your Scrapy project.

    This is what is generated by the `genspider` command:

    ```py
    import scrapy

    class ImdbTopSpider(scrapy.Spider):
        name = "imdb_top"
        allowed_domains = ["imdb.com"]
        start_urls = ["https://imdb.com"]

        def parse(self, response):
            pass
    ```

   

2. Navigate to the spiders folder
    
    `cd imdb_scraper/spiders`
    
 

3. Update the Spider

    Open `imdb_top.py` and update the `start_urls` list to include the IMDb Top 250 Movies page:

    ```py
    start_urls = ["https://www.imdb.com/chart/top/"]
    ``` 

    

4. Populate the Spider

    #### Identifying XPaths for the First Title

        -   Open the IMDb Top 250 Movies page and inspect the first movie title element.
            -   Url to be used: https://www.imdb.com/chart/top/
        -   Use browser developer tools to find the XPath that uniquely identifies the first title. 
            -   This can be done using Google Chrome on Windows by pressing `Ctrl+Shift+C` or on Mac by pressing `Cmd+Shift+C`.
  
        ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/developer_tools.png)
    
        -   Move the mouse until you locate the title of the first movie, then click on it.
  
        ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/first_title.png)
        -   It will highlight the corresponding HTML element in the browser opened by the developer tools.
        
        ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/html_element.png)

        -  Right-click on the highlighted element and select `Copy > Copy XPath` to copy the XPath to the clipboard.

        ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/copy_xpath.png)

        -   The XPath could look something like this:

        `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3`    

        -   You need to add `/text()` to the end of the XPath to extract the text content of the element.

        `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()`

   

    #### Adding XPath to the Spider

    Open `imdb_top.py` and update the `parse` method to include the XPath for extracting the title:

    ```py
    def parse(self, response):
        title_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()'
        title = response.xpath(title_xpath).get().strip()
        yield {'title': title}
    ``` 
    #### Adding custom settings to the Spider to bypass bot detection
    ```py
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    ```
    This can be placed before the `parse` method.
    The `USER_AGENT` is the browser that the spider will use to make the request. In this case, we are using Chrome on Windows 10.

    Your updated spider should look like this:

    ```py
    import scrapy

    class ImdbTopSpider(scrapy.Spider):
        name = "imdb_top"
        allowed_domains = ["imdb.com"]
        start_urls = ["https://www.imdb.com/chart/top/"]
        custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        def parse(self, response):
            title_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()'
            title = response.xpath(title_xpath).get() #.get() is added to extract the text content of the element
            title.strip() #.strip() is added to remove any leading or trailing whitespace
            yield {'title': title}
    ```
   

    #### Testing the Spider

    Run the spider to test if the title is correctly extracted:

    `scrapy crawl imdb_top` 

    Check the terminal output to confirm the successful extraction of the first movie title.

   

    #### Confirming that the Spider Works
    The terminal will display different information from Scrapy that shows the steps it takes to scrape a website.
    ```bash
    2024-01-17 21:53:29 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: imdb_scraper)
    2024-01-17 21:53:29 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.7, Platform Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
    2024-01-17 21:53:29 [scrapy.addons] INFO: Enabled addons:
    []
    2024-01-17 21:53:29 [asyncio] DEBUG: Using selector: EpollSelector
    2024-01-17 21:53:29 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
    2024-01-17 21:53:29 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
    2024-01-17 21:53:29 [scrapy.extensions.telnet] INFO: Telnet Password: e9fa312316a2a6fd
    2024-01-17 21:53:29 [scrapy.middleware] INFO: Enabled extensions:
    ['scrapy.extensions.corestats.CoreStats',
    'scrapy.extensions.telnet.TelnetConsole',
    'scrapy.extensions.memusage.MemoryUsage',
    'scrapy.extensions.logstats.LogStats']
    2024-01-17 21:53:29 [scrapy.crawler] INFO: Overridden settings:
    {'BOT_NAME': 'imdb_scraper',
    'FEED_EXPORT_ENCODING': 'utf-8',
    'NEWSPIDER_MODULE': 'imdb_scraper.spiders',
    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
    'ROBOTSTXT_OBEY': True,
    'SPIDER_MODULES': ['imdb_scraper.spiders'],
    'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    2024-01-17 21:53:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
    ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
    'scrapy.downloadermiddlewares.retry.RetryMiddleware',
    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
    'scrapy.downloadermiddlewares.stats.DownloaderStats']
    2024-01-17 21:53:29 [scrapy.middleware] INFO: Enabled spider middlewares:
    ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
    'scrapy.spidermiddlewares.referer.RefererMiddleware',
    'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
    'scrapy.spidermiddlewares.depth.DepthMiddleware']
    2024-01-17 21:53:29 [scrapy.middleware] INFO: Enabled item pipelines:
    []
    2024-01-17 21:53:29 [scrapy.core.engine] INFO: Spider opened
    2024-01-17 21:53:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2024-01-17 21:53:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
    2024-01-17 21:53:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/robots.txt> (referer: None)
    2024-01-17 21:53:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/chart/top/> (referer: None)
    2024-01-17 21:53:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.imdb.com/chart/top/>
    {'title': '1. The Shawshank Redemption'}
    2024-01-17 21:53:31 [scrapy.core.engine] INFO: Closing spider (finished)
    2024-01-17 21:53:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
    {'downloader/request_bytes': 606,
    'downloader/request_count': 2,
    'downloader/request_method_count/GET': 2,
    'downloader/response_bytes': 291144,
    'downloader/response_count': 2,
    'downloader/response_status_count/200': 2,
    'elapsed_time_seconds': 1.630596,
    'finish_reason': 'finished',
    'finish_time': datetime.datetime(2024, 1, 18, 2, 53, 31, 482837, tzinfo=datetime.timezone.utc),
    'httpcompression/response_bytes': 2789157,
    'httpcompression/response_count': 1,
    'item_scraped_count': 1,
    'log_count/DEBUG': 6,
    'log_count/INFO': 10,
    'memusage/max': 61358080,
    'memusage/startup': 61358080,
    'response_received_count': 2,
    'robotstxt/request_count': 1,
    'robotstxt/response_count': 1,
    'robotstxt/response_status_count/200': 1,
    'scheduler/dequeued': 1,
    'scheduler/dequeued/memory': 1,
    'scheduler/enqueued': 1,
    'scheduler/enqueued/memory': 1,
    'start_time': datetime.datetime(2024, 1, 18, 2, 53, 29, 852241, tzinfo=datetime.timezone.utc)}
    2024-01-17 21:53:31 [scrapy.core.engine] INFO: Spider closed (finished)
    ```

    To locate the extracted title, search for the following line in the terminal output:

    ```bash
    2024-01-17 21:53:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.imdb.com/chart/top/>
    {'title': '1. The Shawshank Redemption'}
    ```

    #### Expanding the Spider to Extract More Details

    Now that the Spider is confirmed to be working and has captured the title correrctly, we can expand the Spider to extract other information availalbe for the same title from the same page.

    Locate the `year`, `duration`, and `rating` for the first title using the browser developer tools and copy the corresponding XPaths to the clipboard.

    ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/content_first_title.png)

    Their xpaths should be:

        - `year`: `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[1]`
        - `duration`: `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[2]`
        - `rating`: `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[3]`



    This can be achieved by updating the `parse` method as follows:

    ```py
    def parse(self, response):
        title_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()'
        year_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[1]/text()'
        duration_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[2]/text()'
        rating_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[3]/text()'
        title = response.xpath(title_xpath).get().strip()
        year = response.xpath(year_xpath).get().strip()
        duration = response.xpath(duration_xpath).get().strip()
        rating = response.xpath(rating_xpath).get().strip()
        yield { 'title': title, 
                'year': year, 
                'duration': duration,
                'rating': rating}
    ```

    An example of the extracted data should look like this:
    ```bash
    2024-01-17 22:10:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.imdb.com/chart/top/>
    {'title': '1. The Shawshank Redemption', 'year': '1994', 'duration': '2h 22m', 'rating': 'R'}
    ```

    Now that we have the data that we wanted to extract, let's clean the `title` field by separating the `rank` from the `title`.
    This can be done by splitting the string using the `'.'` character and taking the second element of the resulting list.
    
        `title = response.xpath(title_xpath).get().strip().split('.')[1].strip()`

    We can also extract the `rank` of the movie by taking the first element of the list.
    
        `rank = response.xpath(title_xpath).get().strip().split('.')[0].strip()`


        - Final version of the `parse` method:
       ```py
        def parse(self, response):
            title_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()'
            year_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[1]/text()'
            duration_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[2]/text()'
            rating_xpath = '//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[2]/span[3]/text()'
            title = response.xpath(title_xpath).get().strip().split('.')[1].strip()
            rank = response.xpath(title_xpath).get().strip().split('.')[0].strip()
            year = response.xpath(year_xpath).get().strip()
            duration = response.xpath(duration_xpath).get().strip()
            rating = response.xpath(rating_xpath).get().strip()
            yield { 'rank': rank,
                    'title': title, 
                    'year': year, 
                    'duration': duration,
                    'rating': rating}
       ```
    
    #### Testing the Spider

    Run the spider to test if the title is correctly extracted:

    `scrapy crawl imdb_top`

    Check the terminal output to confirm the successful extraction of the first movie title.
    
    ```bash
    2024-01-17 23:08:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.imdb.com/chart/top/>
    {'rank': '1', 'title': 'The Shawshank Redemption', 'year': '1994', 'duration': '2h 22m', 'rating': 'R'}
    ```


    
### Step 4: Writing the Spider to extract all Top 250 titles' details

#### Getting the full Top 250 Movies list:
    
   - After extracting the information for the first title, we can alter our code to extract the information for all the titles in the Top 250 Movies list.
   - This can be done by looping through all the titles and extracting the information for each one.
   - Finding the xpath for the entire list
   -  This can be done by inspecting the entire list and copying the xpath of the parent element.

We can extract this by pressing `Ctrl+Shift+C` or on Mac by pressing `Cmd+Shift+C` and clicking on the parent element of the list.

![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/parent_element.png)

Then we can right-click on the highlighted element and select `Copy > Copy XPath` to copy the XPath to the clipboard.

![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/parent_element_xpath.png)

The XPath should look like this:

`//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul`

- Looping through the list
We can loop through the list by using the `for` loop in Python inside the `parse` method.

```py
for movie in response.xpath('//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li'):
```

  - Previously, the xpaths for `title`, `year`, `duration`, and `rating` contained the index of the title in the list.
   
      - Example:
        - `title_xpath` = `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()`

  - We can now compare the xpaths for the first title and the second title to see what is different.
    - First title:
        - `title_xpath` = `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div[1]/a/h3/text()`
    - Second title:
        - `title_xpath` = `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[2]/div[2]/div/div/div[1]/a/h3/text()`
    - The only difference is the index of the title in the list.
    - We can use the `movie` variable from the `for` loop to replace the index in the xpath.
    - The new xpath for the title will be:
        - `title_xpath` = `//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li[2]/div[2]/div/div/div[1]/a/h3/text()`

    - We can now update the `parse` method to extract the information for all the titles in the list.
    - The updated `parse` method should look like this:

    ```py
    def parse(self, response):
        for movie in response.xpath('//*[@id="__next"]/main/div/div[3]/section/div/div[2]/div/ul/li'):
            title_xpath = './div[2]/div/div/div[1]/a/h3/text()'
            year_xpath = './div[2]/div/div/div[2]/span[1]/text()'
            duration_xpath = './div[2]/div/div/div[2]/span[2]/text()'
            rating_xpath = './div[2]/div/div/div[2]/span[3]/text()'
            title = movie.xpath(title_xpath).get().strip().split('.')[1].strip()
            rank = movie.xpath(title_xpath).get().strip().split('.')[0].strip()
            year = movie.xpath(year_xpath).get().strip()
            duration = movie.xpath(duration_xpath).get().strip()
            rating = movie.xpath(rating_xpath)
            # check if rating exists before extracting since some titles don't have a rating
            if rating:
                rating = rating.get().strip()
            yield { 'rank': rank,
                    'title': title, 
                    'year': year, 
                    'duration': duration,
                    'rating': rating}
    ```

#### `rating` will need to be checked if it exists before extracting since some titles don't have a rating. 

### Testing the updated Spider

- Run the spider to test if the title is correctly extracted:

    `scrapy crawl imdb_top`

###  Step 5: Saving the output to a CSV file

- We can save the output of the spider to a CSV file by using the `-o` flag when running the spider.
- The `-o` flag takes the name of the output file as an argument.
- The output file will be created in the same directory as the spider.
- Example:
    - `scrapy crawl imdb_top -o movies.csv`

#### Running the Spider

- Run the spider to test if the title is correctly extracted:

    `scrapy crawl imdb_top -o movies.csv`
    
- The csv file should be created in the same directory as the spider.
  - It will look like the following:

    ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/csv_output.png)

**You have now successfully created a spider that can extract the Top 250 movies from IMDb and save them to a CSV file.**

---

### (Optional) Visualizing the data using Pandas
#### Installing Pandas
- We can use Pandas to read the CSV file and visualize the data.
- In order to use Pandas, we need to install it in our virtual environment.
- We can install Pandas using the following command:
    - `pip install pandas`

#### Using Pandas to read the CSV file
- Start by creating a new file named `read_csv.py` in the same directory as the spider.
  - You can run the script using the following command:
    - `python read_csv.py`
- Add the following code to the file:
    ```py
    import pandas as pd
    df = pd.read_csv('movies.csv')
    ```
- This will read the CSV file and store it in a Pandas DataFrame named `df`.

#### Using Pandas to display the data
##### Displaying the top 10 years
- We can  use Pandas to display the number of movies per year.
    ```py
    import pandas as pd
    df = pd.read_csv('movies.csv')
    df_top10 = df['year'].value_counts().head(10)
    print(df_top10)
    ```
- This will display the number of movies per year in descending order.
- The output should look like:
  
    ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/top_10_years.png)

##### Displaying the number of movies per rating

- We can  use Pandas to display the number of movies per rating.
    ```py
    import pandas as pd
    df = pd.read_csv('movies.csv')
    df_top10 = df['rating'].value_counts()
    print(df_top10)
    ```
- This will display the number of movies per rating in descending order.

- The output should look like:
  
    ![Alt text](https://s3.amazonaws.com/weclouddata/images/data_engineer/top_10_ratings.png)

---
    
### You may also use other Python libraries to visualize the data such as `Matplotlib`. You will need to install the libraries in your virtual environment before using them.


-- 
You can download the full demo code from this [link](https://s3.amazonaws.com/weclouddata/data/data/imdb_top.py) 
